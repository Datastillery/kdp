<?xml version="1.0"?>
<?xml-stylesheet type="text/xsl" href="configuration.xsl"?>
<configuration>
  <property>
    <name>hive.metastore.uris</name>
    <value>thrift://hive-metastore:9083</value>
  </property>
  <property>
    <name>hive.metastore.warehouse.dir</name>
    <value>s3a://jeff-jarred-751/hive-s3</value>
  </property>
  <property>
    <name>javax.jdo.option.ConnectionURL</name>
    <value>{{ .Values.kylo.services.properties.hive.metastore.datasource.url }}</value>
    <description>metadata is stored in a PostgreSQL server</description>
  </property>
  <property>
    <name>javax.jdo.option.ConnectionDriverName</name>
    <value>org.postgresql.Driver</value>
    <description>PostgreSQL JDBC driver class</description>
  </property>
  <property>
    <name>javax.jdo.option.ConnectionUserName</name>
    <value>{{ .Values.kylo.services.properties.hive.metastore.datasource.username }}</value>
    <description>user name for connecting to pgsql server</description>
  </property>
  <property>
    <name>javax.jdo.option.ConnectionPassword</name>
    <value>{{ .Values.kylo.services.properties.hive.metastore.datasource.password }}</value>
    <description>password for connecting to pgsql server</description>
  </property>
  <property>
    <name>hive.metastore.schema.verification</name>
    <value>false</value>
  </property>
  <property>
    <name>hive.metastore.schema.verification.record.version</name>
    <value>false</value>
    <description>Creates necessary schema on a startup if one doesn't exist</description>
  </property>
  <property>
    <name>datanucleus.autoCreateSchema</name>
    <value>true</value>
    <description>Creates necessary schema on a startup if one doesn't exist</description>
  </property>
  <property>
    <name>datanucleus.fixedDatastore</name>
    <value>true</value>
  </property>
  <property>
    <name>datanucleus.autoCreateTables</name>
    <value>True</value>
  </property>
  <property>
    <name>hive.execution.engine</name>
    <value>spark</value>
  </property>
  <property>
    <name>hive.support.concurrency</name>
    <value>true</value>
  </property>
  <property>
    <name>hive.exec.dynamic.partition.mode</name>
    <value>nonstrict</value>
  </property>
  <property>
    <name>hive.txn.manager</name>
    <value>org.apache.hadoop.hive.ql.lockmgr.DbTxnManager</value>
  </property>
  <property>
    <name>hive.metastore.event.db.notification.api.auth</name>
    <value>false</value>
  </property>
  <property>
    <name>spark.master</name>
    <value>k8s://https://kubernetes.default.svc.cluster.local:443</value>
  </property>
  <property>
    <name>spark.eventLog.enabled</name>
    <value>true</value>
  </property>
  <property>
    <name>spark.eventLog.dir</name>
    <value>/tmp</value>
  </property>
  <property>
    <name>spark.executor.memory</name>
    <value>512m</value>
  </property>
  <property>
    <name>spark.serializer</name>
    <value>org.apache.spark.serializer.KryoSerializer</value>
  </property>
  <property>
    <name>spark.kubernetes.namespace</name>
    <value>default</value>
  </property>
  <property>
    <name>spark.kubernetes.container.image</name>
    <value>jeffgrunewald/scos-spark:1.11</value>
  </property>
  <property>
    <name>spark.kubernetes.container.image.pullPolicy</name>
    <value>IfNotPresent</value>
  </property>
  <property>
    <name>spark.executor.instances</name>
    <value>2</value>
  </property>
  <property>
    <name>spark.kubernetes.allocation.batch.size</name>
    <value>2</value>
  </property>
  <property>
    <name>spark.kubernetes.allocation.batch.delay</name>
    <value>10s</value>
  </property>
  <property>
    <name>hive.spark.client.rpc.server.address</name>
    <value>POD_IP.default.pod.cluster.local</value>
  </property>
  <property>
    <name>spark.kubernetes.driver.label.seatOf</name>
    <value>pants</value>
  </property>
  <property>
    <name>spark.kubernetes.executor.label.seatOf</name>
    <value>pants</value>
  </property>
  <property>
    <name>spark.app.name</name>
    <value>de-doop</value>
  </property>
  <property>
    <name>spark.submit.deployMode</name>
    <value>cluster</value>
  </property>
  <property>
    <name>spark.driver.extraLibraryPath</name>
    <value>/opt/hadoop/lib/native</value>
  </property>
  <property>
    <name>spark.executor.extraJavaOptions</name>
    <value>-Dlog4j.configuration=file:///opt/spark/conf/log4j.properties</value>
  </property>
</configuration>
